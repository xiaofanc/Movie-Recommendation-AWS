{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import countDistinct, explode, split, concat_ws, collect_list, isnan\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# STEP 1: Get the params of the created redshift cluster \n",
    "- We need:\n",
    "    - The redshift cluster <font color='red'>endpoint</font>\n",
    "    - The <font color='red'>IAM role ARN</font> that give access to Redshift to read from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['KEY']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# FILL IN THE REDSHIFT ENPOINT HERE\n",
    "# e.g. DWH_ENDPOINT=\"redshift-cluster-1.csmamz5zxmle.us-west-2.redshift.amazonaws.com\" \n",
    "DWH_ENDPOINT=\"dwhcluster.ci2m6m74tbzm.us-west-2.redshift.amazonaws.com\" \n",
    "    \n",
    "#FILL IN THE IAM ROLE ARN you got in step 2.2 of the previous exercise\n",
    "#e.g DWH_ROLE_ARN=\"arn:aws:iam::988332130976:role/dwhRole\"\n",
    "DWH_ROLE_ARN=\"arn:aws:iam::264680862608:role/dwhRole\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Part 1: Load Data from S3 and clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwhcluster.ci2m6m74tbzm.us-west-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='udacity-input', key='ml-latest-small/')\n",
      "s3.ObjectSummary(bucket_name='udacity-input', key='ml-latest-small/Awards.txt')\n",
      "s3.ObjectSummary(bucket_name='udacity-input', key='ml-latest-small/links.csv')\n",
      "s3.ObjectSummary(bucket_name='udacity-input', key='ml-latest-small/movies.csv')\n",
      "s3.ObjectSummary(bucket_name='udacity-input', key='ml-latest-small/ratings.csv')\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                     )\n",
    "\n",
    "s3bucket =  s3.Bucket(\"udacity-input\") # private\n",
    "\n",
    "s3_data = iter(s3bucket.objects.filter(Prefix=\"ml-latest-small\"))\n",
    "for _ in range(5): print(next(s3_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movieSchema = R([\n",
    "            Fld(\"movieId\",Int()),\n",
    "            Fld(\"title\",Str()),\n",
    "            Fld(\"genres\",Str())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ratingSchema = R([\n",
    "            Fld(\"userId\",Int()),\n",
    "            Fld(\"movieId\",Int()),\n",
    "            Fld(\"rating\",Dbl()),\n",
    "            Fld(\"ts\",Str())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tagSchema = R([\n",
    "            Fld(\"userId\",Int()),\n",
    "            Fld(\"movieId\",Int()),\n",
    "            Fld(\"tag\",Str()),\n",
    "            Fld(\"ts\",Str())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read movies, ratings, and tags csv\n",
    "dfmovies = spark.read.csv(\"s3a://udacity-input/ml-latest-small/movies.csv\", header=True, schema=movieSchema)\n",
    "dfratings = spark.read.csv(\"s3a://udacity-input/ml-latest-small/ratings.csv\", header = True, schema=ratingSchema)\n",
    "dftags = spark.read.csv(\"s3a://udacity-input/ml-latest-small/tags.csv\", header = True, schema=tagSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+--------+----------+-----------+\n",
      "|Film                                                    |Year    |Awards    |Nominations|\n",
      "+--------------------------------------------------------+--------+----------+-----------+\n",
      "|Parasite                                                |2019    |4         |6.0        |\n",
      "|Ford v Ferrari                                          |2019    |2         |4.0        |\n",
      "|Learning to Skateboard in a Warzone (If You're a Girl)  |2019    |1         |1.0        |\n",
      "|The Neighbors' Window                                   |2019    |1         |1.0        |\n",
      "|Little Women                                            |2019    |1         |6.0        |\n",
      "|Marriage Story                                          |2019    |1         |6.0        |\n",
      "|Jojo Rabbit                                             |2019    |1         |6.0        |\n",
      "|Toy Story 4                                             |2019    |1         |2.0        |\n",
      "|Joker   2019                                            |2       |1         |1.0        |\n",
      "|Once Upon a Time in Hollywood   2019                    |2       |1         |0.0        |\n",
      "+--------------------------------------------------------+--------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read awards txt\n",
    "dfawards = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"s3a://udacity-input/ml-latest-small/Awards.txt\")\n",
    "\n",
    "dfawards.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read award_corrected txt\n",
    "dfawards2 = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"s3a://udacity-input/ml-latest-small/Award_corrected.txt\")\n",
    "\n",
    "dfawards2.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|movieId|title                             |genres                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9742"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmovies.printSchema()\n",
    "dfmovies.show(5, truncate = False)\n",
    "dfmovies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfratings = dfratings.withColumn(\n",
    "    \"rate_time\",\n",
    "    F.to_timestamp(F.from_unixtime((col(\"ts\") / 1000) , 'yyyy-MM-dd HH:mm:ss.SSS')).cast(\"Timestamp\")\n",
    ").drop(\"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- rate_time: timestamp (nullable = true)\n",
      "\n",
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          rate_time|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      1|   4.0|1970-01-12 04:03:02|\n",
      "|     1|      3|   4.0|1970-01-12 04:03:01|\n",
      "|     1|      6|   4.0|1970-01-12 04:03:02|\n",
      "|     1|     47|   5.0|1970-01-12 04:03:03|\n",
      "|     1|     50|   5.0|1970-01-12 04:03:02|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfratings.printSchema()\n",
    "dfratings.show(5)\n",
    "dfratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dftags = dftags.withColumn(\"tag_time\", F.to_timestamp(col(\"ts\") / 1000)).drop(\"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- tag_time: timestamp (nullable = true)\n",
      "\n",
      "+------+-------+---------------+--------------------+\n",
      "|userId|movieId|            tag|            tag_time|\n",
      "+------+-------+---------------+--------------------+\n",
      "|     2|  60756|          funny|1970-01-17 17:35:...|\n",
      "|     2|  60756|Highly quotable|1970-01-17 17:35:...|\n",
      "|     2|  60756|   will ferrell|1970-01-17 17:35:...|\n",
      "|     2|  89774|   Boxing story|1970-01-17 17:35:...|\n",
      "|     2|  89774|            MMA|1970-01-17 17:35:...|\n",
      "+------+-------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3683"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftags.printSchema()\n",
    "dftags.show(5)\n",
    "dftags.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Film   ', 'Year   ', 'Awards    ', 'Nominations']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfawards.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards = dfawards.withColumn(\"title\", dfawards['Film   '].cast(Str())).drop('Film   ')\n",
    "dfawards = dfawards.withColumn(\"year\", dfawards['Year   '].cast(Date())).drop(\"Year   \")\n",
    "dfawards = dfawards.withColumn(\"awards\", dfawards['Awards    '].cast(Dbl())).drop(\"Awards    \")\n",
    "dfawards = dfawards.withColumn(\"nominations\", dfawards['Nominations'].cast(Int()))\n",
    "dfawards = dfawards.withColumn(\"year\", F.year(\"year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards2 = dfawards2.withColumn(\"title\", dfawards2['Film   '].cast(Str())).drop('Film   ')\n",
    "dfawards2 = dfawards2.withColumn(\"year\", dfawards2['Year   '].cast(Date())).drop(\"Year   \")\n",
    "dfawards2 = dfawards2.withColumn(\"awards\", dfawards2['Awards    '].cast(Dbl())).drop(\"Awards    \")\n",
    "dfawards2 = dfawards2.withColumn(\"nominations\", dfawards2['Nominations'].cast(Int()))\n",
    "dfawards2 = dfawards2.withColumn(\"year\", F.year(\"year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nominations: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- awards: double (nullable = true)\n",
      "\n",
      "+-----------+--------------------+----+------+\n",
      "|nominations|               title|year|awards|\n",
      "+-----------+--------------------+----+------+\n",
      "|          6|        Parasite    |2019|   4.0|\n",
      "|          4|    Ford v Ferrari  |2019|   2.0|\n",
      "|          1|Learning to Skate...|2019|   1.0|\n",
      "|          1|The Neighbors' Wi...|2019|   1.0|\n",
      "|          6|    Little Women    |2019|   1.0|\n",
      "+-----------+--------------------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1316"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfawards.printSchema()\n",
    "dfawards.show(5)\n",
    "dfawards.count()\n",
    "dfawards2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nominations: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- awards: double (nullable = true)\n",
      "\n",
      "+-----------+--------------------+----+------+\n",
      "|nominations|               title|year|awards|\n",
      "+-----------+--------------------+----+------+\n",
      "|          6|        Parasite    |2019|   4.0|\n",
      "|          4|    Ford v Ferrari  |2019|   2.0|\n",
      "|          1|Learning to Skate...|2019|   1.0|\n",
      "|          1|The Neighbors' Wi...|2019|   1.0|\n",
      "|          6|    Little Women    |2019|   1.0|\n",
      "+-----------+--------------------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1316"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfawards.printSchema()\n",
    "dfawards.show(5)\n",
    "dfawards.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Part 2: Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Wrangling with DataFrames\n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check for null values\n",
    "dfmovies.columns.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### How to deal with null values\n",
    "#### 1. Deleting Rows \n",
    "This method commonly used to handle the null values. Here, we either delete a particular row if it has a null value for a particular feature and a particular column if it has more than 70-75% of missing values. This method is advised only when there are enough samples in the data set. \n",
    "#### 2. Replacing With Mean/Median/Mode  \n",
    "This strategy can be applied on a feature which has numeric data like the age of a person or the rating score. We can calculate the mean, median or mode of the feature and replace it with the missing values. This is an approximation which can add variance to the data set. \n",
    "#### 3. Assigning An Unique Category  \n",
    "A categorical feature will have a definite number of possibilities, such as gender, for example. Since they have a definite number of classes, we can assign another class for the missing values like unknown.\n",
    "#### 4. Predicting The Missing Values  \n",
    "Using the features which do not have missing values, we can predict the nulls with the help of a machine learning algorithm. \n",
    "#### 5. Using Algorithms Which Support Missing Values  \n",
    "KNN is a machine learning algorithm which works on the principle of distance measure. This algorithm can be used when there are nulls present in the dataset. While the algorithm is applied, KNN considers the missing values by taking the majority of the K nearest values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check duplicate data, also confirm the dataset is on which level\n",
    "dfmovies.shape\n",
    "dfmovies[['movieId']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfratings.shape\n",
    "dfratings[['movieId', 'userId']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfawards.shape\n",
    "dfawards[['movieId', 'year']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# basic count\n",
    "# number of movies in the dataset\n",
    "distinct_movie = dfmovies['movieId'].nunique()\n",
    "print('{} movies in the movies dataset'.format(distinct_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of users in the dataset\n",
    "distinct_user = dfratings['userId'].nunique()\n",
    "print('{} users rated the movies'.format(distinct_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of movies receiving awards\n",
    "distinct_award = dfawards['title'].nunique()\n",
    "print('{} movies received awards'.format(distinct_award))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Minimum number of ratings per user\n",
    "# Minimum number of ratings per movie \n",
    "tmp1 = dfratings.groupBy(\"userID\").count().toPandas()['count'].min()\n",
    "tmp2 = dfratings.groupBy(\"movieId\").count().toPandas()['count'].min()\n",
    "print('For the users that rated movies and the movies that were rated:')\n",
    "print('Minimum number of ratings per user is {}'.format(tmp1))\n",
    "print('Minimum number of ratings per movie is {}'.format(tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of movies rated by only one user\n",
    "tmp1 = sum(dfratings.groupBy(\"movieId\").count().toPandas()['count'] == 1)\n",
    "tmp2 = dfratings.select('movieId').distinct().count()\n",
    "print('{} out of {} movies are rated by only one user'.format(tmp1, tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check the dates in the ratings dataset\n",
    "tmp1 = dfratings['rate_time'].min()\n",
    "tmp2 = dfratings['rate_time'].max()\n",
    "print('ratings were made during {} and {}'.format(tmp1, tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split the mixed genres by '|'\n",
    "dfmovies2 = dfmovies.withColumn('genre', explode(split(dfmovies.genres, '\\|')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfmovies2.show(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[genre: string, title: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the dataframe dfmovies2 produced to match every movie to a single genre\n",
    "genre_movies = dfmovies2 \\\n",
    "                    .groupBy(dfmovies2.genre) \\\n",
    "                    .agg(concat_ws(',', collect_list(dfmovies2.title)) \\\n",
    "                    .alias('title')) \\\n",
    "                    .orderBy('genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use the dataframe dfmovies2 produced to count movie in a single genre\n",
    "genre_movies = dfmovies2 \\\n",
    "                    .groupBy(dfmovies2.genre) \\\n",
    "                    .sum(dfmovies2.title) \\\n",
    "                    .alias('count')) \\\n",
    "                    .orderBy('genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|             genre|               title|\n",
      "+------------------+--------------------+\n",
      "|(no genres listed)|La cravate (1957)...|\n",
      "|            Action|Heat (1995),Sudde...|\n",
      "|         Adventure|Toy Story (1995),...|\n",
      "|         Animation|Toy Story (1995),...|\n",
      "|          Children|Toy Story (1995),...|\n",
      "|            Comedy|Toy Story (1995),...|\n",
      "|             Crime|Heat (1995),Casin...|\n",
      "|       Documentary|Nico Icon (1995),...|\n",
      "|             Drama|Waiting to Exhale...|\n",
      "|           Fantasy|Toy Story (1995),...|\n",
      "|         Film-Noir|Devil in a Blue D...|\n",
      "|            Horror|Dracula: Dead and...|\n",
      "|              IMAX|Apollo 13 (1995),...|\n",
      "|           Musical|Pocahontas (1995)...|\n",
      "|           Mystery|Copycat (1995),Ci...|\n",
      "|           Romance|Grumpier Old Men ...|\n",
      "|            Sci-Fi|Powder (1995),Cit...|\n",
      "|          Thriller|Heat (1995),Golde...|\n",
      "|               War|Richard III (1995...|\n",
      "|           Western|Desperado (1995),...|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genre_movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[genre: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pick distinct genre\n",
    "genres_dummies = dfmovies2.filter(dfmovies2.genre != '(no genres listed)') \\\n",
    "                     .select(dfmovies2.genre).distinct() \\\n",
    "                     .orderBy(dfmovies2.genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      genre|\n",
      "+-----------+\n",
      "|     Action|\n",
      "|  Adventure|\n",
      "|  Animation|\n",
      "|   Children|\n",
      "|     Comedy|\n",
      "|      Crime|\n",
      "|Documentary|\n",
      "|      Drama|\n",
      "|    Fantasy|\n",
      "|  Film-Noir|\n",
      "|     Horror|\n",
      "|       IMAX|\n",
      "|    Musical|\n",
      "|    Mystery|\n",
      "|    Romance|\n",
      "|     Sci-Fi|\n",
      "|   Thriller|\n",
      "|        War|\n",
      "|    Western|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genres_dummies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Wrangling with Spark SQL and OLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfratings.createOrReplaceTempView(\"ratings\")     #userid, movieid, rating, timestamp\n",
    "dfmovies.createOrReplaceTempView(\"movies\")       #movieid, title, genres\n",
    "dflinks.createOrReplaceTempView(\"links\")         #movieid, imdbId, tmdbId\n",
    "dftags.createOrReplaceTempView(\"tags\")           #userid, movieid, tag, timestamp\n",
    "dfawards.createOrReplaceTempView(\"awards\")       #title, year, awards, nominations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split title and release year in separate columns in movies dataframe. Convert year to timestamp.       \n",
    "movies = spark.sql(\"select movieId, substr(title, 0, length(title)-7) as title, substr(title, -5, 4) as year from movies\")\n",
    "movies.show()\n",
    "movies.createOrReplaceTempView(\"movies\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of movies not rated\n",
    "spark.sql(\"\"\"select \n",
    "          count(distinct movies.movieId)\n",
    "          from movies \n",
    "          where movies.movieId not in\n",
    "          (select distinct ratings.movieId from ratings)\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the top 5 movies with high ratings\n",
    "spark.sql(\"\"\"select distinct\n",
    "    m.title as title,\n",
    "    avg(r.rating) as avg_rating\n",
    "    from movies as m join ratings as r on m.movieId = r.movieId\n",
    "    group by m.title\n",
    "    order by avg_rating desc\n",
    "    limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the most awards a movie got\n",
    "spark.sql(\"\"\"select distinct\n",
    "                    title,\n",
    "                    sum(awards) as tot_awards\n",
    "                    from awards\n",
    "                    group by title\n",
    "                    order by tot_awards desc\n",
    "                    limit 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the rating score of movie with highest awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# year of awards in the dataset\n",
    "spark.sql(\"\"\"select \n",
    "             min(year) as min_year,\n",
    "             max(year) as max_year\n",
    "             from awards\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# STEP 3: Connect to the Redshift Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Part 1: Extract data and transform into fact and dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS dimDate\n",
    "CREATE TABLE dimDate\n",
    "(\n",
    "  date_key timestamp NOT NULL PRIMARY KEY,\n",
    "  year smallint NOT NULL,\n",
    "  month smallint NOT NULL,\n",
    "  day smallint NOT NULL,\n",
    "  week smallint NOT NULL,\n",
    "  weekday varchar(3) NOT NULL\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS dimRating\n",
    "CREATE TABLE dimRating\n",
    "(\n",
    "  userId             smallint NOT NULL PRIMARY KEY,\n",
    "  movieId            smallint NOT NULL,\n",
    "  rating             numeric NOT NULL,\n",
    "  rate_time          timestamp REFERENCES dimdate (date_key)\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS dimGenres\n",
    "CREATE TABLE dimGenres\n",
    "(\n",
    "  genreId            smallint NOT NULL PRIMARY KEY\n",
    "  genres             text NOT NULL,\n",
    "  title              varchar(45) NOT NULL\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS factMovies\n",
    "CREATE TABLE factMovies\n",
    "(\n",
    "  MovieId      smallint NOT NULL PRIMARY KEY,\n",
    "  title        varchar(45) NOT NULL,\n",
    "  release_year year NOT NULL,\n",
    "  awards       smallint NOT NULL,\n",
    "  Nominations  smallint NOT NULL\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dimDate (date_key, date, year, month, day, week, dow)\n",
    "SELECT DISTINCT(TO_CHAR(timestamp :: DATE, 'yyyyMMDD')::integer) AS date_key,\n",
    "       date(timestamp)                                           AS date,\n",
    "       EXTRACT(year FROM timestamp)                              AS year,\n",
    "       EXTRACT(month FROM timestamp)                             AS month,\n",
    "       EXTRACT(day FROM timestamp)                               AS day,\n",
    "       EXTRACT(week FROM timestamp)                              AS week,\n",
    "       dayofweek(timestamp)                                      AS dow\n",
    "FROM ratings;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO factMovies (movieId, title, release_year, awards, nominations)\n",
    "SELECT DISTINCT(movieId)                                         AS movieId,\n",
    "       title                                                     AS title,\n",
    "       year                                                      AS release_year,\n",
    "       awards                                                    AS awards,\n",
    "       nominations                                               AS nominations\n",
    "FROM movies as m LEFT JOIN awards as a on upper(trim(m.title)) = upper(trim(a.title));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dimRatings (userId, movieId, rating, rate_time)\n",
    "SELECT * FROM ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dimGenres (genreId, genre, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_field = [\"title\", \"duration\", \"year\", \"artist_id\"]\n",
    "songs_table = df.select(song_field).dropDuplicates().withColumn(\"song_id\", F.monotonically_increasing_id()).filter(~col(\"year\").isin([0]) & col(\"year\").isNotNull() & col(\"artist_id\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artist_field = [\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"]\n",
    "artists_table = df.select(artist_field).dropDuplicates().dropna(subset=[\"artist_id\",\"artist_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(\"s3a://sparkifytest/songs/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artists_table.write.parquet(\"s3a://sparkifytest/artists/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_field = [\" userId as user_id\", \"firstName as first_name\", \"lastName as last_name\", \"gender\", \"level\"]\n",
    "users_table = df.selectExpr(user_field).dropDuplicates().dropna(how = \"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "users_table.write.parquet(\"s3a://sparkifytest/users/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table.write.partitionBy(\"year\", \"month\").parquet(\"s3a://sparkifytest/time/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_df = spark.read.parquet(\"s3a://sparkifytest/songs/*/*/*\")\n",
    "artist_df = spark.read.parquet(\"s3a://sparkifytest/artists/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Join_song = df.join(song_df, ((song_df.title == df.song) & (song_df.duration == df.length)))\n",
    "artists_songs_logs = Join_song.join(artist_df, (Join_song.artist == artist_df.artist_name))\n",
    "songplays = artists_songs_logs.join(time_table, (artists_songs_logs.start_time == time_table.start_time), 'left').drop(artists_songs_logs.start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_field = [\"start_time\", \"userId as user_id\", \"level\", \"song_id\", \"artist_id\", \"sessionid as session_id\", \"artist_location as location\", \"userAgent as user_agent\", \"year\", \"month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table = songplays.selectExpr(songplays_field).dropDuplicates().dropna(subset=[\"user_id\", \"artist_id\", \\\n",
    "\"song_id\"]).withColumn(\"songplay_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table.write.partitionBy(\"year\", \"month\").parquet(output_date + \"songplays/\", mode=\"overwrite\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table.createOrReplaceTempView(\"songplays\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT month, count(song_id) as song_num\n",
    "    FROM songplays\n",
    "    GROUP by month\n",
    "    order by song_num desc\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
